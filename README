
TP2 : Clustering et DÃ©tection d'Anomalies

Ce projet explore diffÃ©rentes techniques d'apprentissage non supervisÃ© appliquÃ©es Ã  deux domaines distincts : 
le clustering de donnÃ©es gÃ©nomiques (Hi-Seq) et la dÃ©tection d'anomalies sur des donnÃ©es mÃ©dicales (ECG).

Le projet est structurÃ© de maniÃ¨re modulaire, sÃ©parant le code source (fonctions utilitaires, modÃ¨les) des notebooks d'analyse et d'exÃ©cution.


.
â”œâ”€â”€ data/                   # Dossier des donnÃ©es (brutes et prÃ©traitÃ©es)
â”‚   â”œâ”€â”€ HISEQ/              # DonnÃ©es pour le clustering
â”‚   â””â”€â”€ ECG/                # DonnÃ©es pour la dÃ©tection d'anomalies
â”œâ”€â”€ notebooks/              # Notebooks Jupyter d'exÃ©cution
â”‚   â”œâ”€â”€ PreparationData.ipynb             # Ã€ exÃ©cuter en premier (normalisation)
â”‚   â”œâ”€â”€ clustering/
â”‚   â”‚   â”œâ”€â”€ Clustering_SIMPLE.ipynb       # Clustering sur donnÃ©es brutes
â”‚   â”‚   â”œâ”€â”€ Clustering_PCA.ipynb          # Clustering aprÃ¨s PCA
â”‚   â”‚   â””â”€â”€ Clustering_UMAP.ipynb         # Clustering aprÃ¨s UMAP
â”‚   â””â”€â”€ anomalies/
â”‚       â”œâ”€â”€ Anomalie_IsolationForest.ipynb # DÃ©tection via Isolation Forest
â”‚       â””â”€â”€ Anomalie_AutoEncodeur.ipynb    # DÃ©tection via AE et DAE (PyTorch)
â”œâ”€â”€ src/                    # Code source modulaire
â”‚   â”œâ”€â”€ models.py           # Architectures PyTorch (Autoencoder, DAE)
â”‚   â”œâ”€â”€ preprocess.py       # Fonctions de normalisation et rÃ©duction de dimension
â”‚   â”œâ”€â”€ train.py            # Boucles d'entraÃ®nement PyTorch
â”‚   â””â”€â”€ utils.py            # Chargement de donnÃ©es, mÃ©triques et visualisations
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


ğŸ› ï¸ PrÃ©requis et Installation

Le projet nÃ©cessite Python 3.8+ et les bibliothÃ¨ques suivantes :

    numpy, pandas (Manipulation de donnÃ©es)
    scikit-learn (Algorithmes de ML classiques)
    torch (Deep Learning)
    umap-learn (RÃ©duction de dimension UMAP)
    kneed (DÃ©tection du coude pour DBSCAN/K-Means)
    matplotlib, seaborn (Visualisation)


Vous pouvez installer les dÃ©pendances via pip :

pip install numpy pandas scikit-learn torch umap-learn kneed matplotlib seaborn


ğŸš€ Instructions d'Utilisation

1. PrÃ©paration des DonnÃ©es (Ã‰tape Obligatoire)

Avant de lancer les analyses, vous devez prÃ©parer et normaliser les donnÃ©es brutes.

    Assurez-vous que les fichiers bruts (hiseq_data.csv, hiseq_labels.csv, ecg.npz) sont dans le dossier data/.
    ExÃ©cutez le notebook : notebooks/PreparationData.ipynb, Cela gÃ©nÃ©rera les fichiers .npy normalisÃ©s nÃ©cessaires pour la suite.

2. Partie Clustering (DonnÃ©es Hi-Seq)

L'objectif est de regrouper des Ã©chantillons de tumeurs selon leur expression gÃ©nique. Trois approches sont comparÃ©es :

    Sans rÃ©duction : notebooks/clustering/Clustering_SIMPLE.ipynb
    Avec PCA (100D) : notebooks/clustering/Clustering_PCA.ipynb
    Avec UMAP (100D) : notebooks/clustering/Clustering_UMAP.ipynb

Chaque notebook teste 3 algorithmes :

    K-Means (mÃ©thode des centres)
    DBSCAN (mÃ©thode basÃ©e sur la densitÃ©)
    Spectral Clustering (mÃ©thode basÃ©e sur les graphes)

Les rÃ©sultats sont Ã©valuÃ©s via des mÃ©triques internes (Silhouette) et externes (ARI, V-Measure) sur 10 exÃ©cutions pour garantir la robustesse statistique.

3. Partie DÃ©tection d'Anomalies (DonnÃ©es ECG)

L'objectif est d'identifier des battements cardiaques anormaux dans des signaux ECG.

    Approche Statistique : notebooks/anomalies/Anomalie_IsolationForest.ipynb
        Utilise l'algorithme Isolation Forest.

    Approche Deep Learning : notebooks/anomalies/Anomalie_AutoEncodeur.ipynb
        Compare un Auto-Encodeur (AE) classique et un Denoising Auto-Encodeur (DAE).
        Le modÃ¨le apprend Ã  reconstruire les donnÃ©es saines ; les anomalies sont dÃ©tectÃ©es via une erreur de reconstruction Ã©levÃ©e (MSE).


ğŸ§  DÃ©tails Techniques (Dossier src/)

Le code a Ã©tÃ© factorisÃ© pour Ã©viter la redondance dans les notebooks :

    src/preprocess.py : Contient normalize_data, reduce_dimension_for_clustering (PCA/UMAP) et reduce_dimension_for_viz.
    src/models.py : DÃ©finit les classes PyTorch Autoencoder et DenoisingAutoencoder.
    src/train.py : GÃ¨re les boucles d'entraÃ®nement, la gestion du bruit pour le DAE et le calcul de la perte.

    src/utils.py :

        load_dataset : Chargement robuste des CSV.
        evaluate_clustering : Calcul centralisÃ© des scores (ARI, Silhouette, HomogÃ©nÃ©itÃ©, etc.).
        run_stochastic_protocol : Lance N exÃ©cutions pour obtenir moyenne et Ã©cart-type.
        plot_clusters : Visualisation 2D des rÃ©sultats.